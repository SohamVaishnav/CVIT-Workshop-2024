{"cells":[{"cell_type":"markdown","metadata":{"id":"258SyDtqd5B9"},"source":["# MNIST Digit Recognition Workbook"]},{"cell_type":"markdown","metadata":{"id":"JgODydrRd5CA"},"source":["### Starter Code\n","\n","First, let's load the MNIST dataset and split it into training and testing sets."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":59890,"status":"ok","timestamp":1716404517076,"user":{"displayName":"Soham Vaishnav","userId":"12778674038887209920"},"user_tz":-330},"id":"6t_6wOH2d5CA","outputId":"90954637-27af-4090-da6f-a0d559f6ccf5"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n","  warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training set shape: (56000, 784)\n","Test set shape: (14000, 784)\n"]}],"source":["from sklearn.datasets import fetch_openml\n","from sklearn.model_selection import train_test_split\n","\n","# Load MNIST data from https://openml.org/d/554\n","X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n","\n","# Split data into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","print(\"Training set shape:\", X_train.shape)\n","print(\"Test set shape:\", X_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"Ks20XpdFd5CB"},"source":["### Visualization and Plotting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PX7bmHO7d5CC"},"outputs":[],"source":["from matplotlib import pyplot as plt\n","print(X_test[X_test.columns[1]])"]},{"cell_type":"markdown","metadata":{"id":"ZhKVNwI-d5CC"},"source":["## Part 1: PCA + KNN\n","\n","### Prelude\n","\n","Principal Component Analysis (PCA) is a statistical technique to emphasize variation and bring out strong patterns in a dataset. It's often used to make data easy to explore and visualize. Here, you will use PCA to reduce the dimensionality of the MNIST dataset before applying the KNN algorithm for classification.\n","\n","### Steps\n","\n","1. **Load the Dataset:** Start by loading the MNIST dataset.\n","2. **Apply PCA:** Reduce the dimensionality of the dataset.\n","3. **KNN Classification:** Use the KNN algorithm to classify the digits.\n","\n","# Tips\n","- Choosing n_components for PCA: Start with n_components=0.95 which keeps 95% of the variance. Experiment with other values to see how it changes the results.\n","- Choosing n_neighbors for KNN: Common starting points are 3, 5, and 7. Adjust based on the performance and try to avoid overfitting.\n","- Explore: Use visualizations like plotting some of the digits before and after PCA to understand what is retained and what is lost."]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":13080,"status":"ok","timestamp":1716404535480,"user":{"displayName":"Soham Vaishnav","userId":"12778674038887209920"},"user_tz":-330},"id":"jnzzTTHKd5CC"},"outputs":[],"source":["#@title Feature Standardization\n","#this is required because methods such as KNN, K-means, and regression are optimisation techniques that aim to\n","#minimise the distance (in some sense). This implies that each feature gets a different importance and therefore\n","#has a different effect on the data above -> more prominance to features with higher values\n","from sklearn.preprocessing import StandardScaler\n","\n","X_test_cols = X_test.columns\n","X_test_num_cols= X_test.shape[1]\n","for i in range(X_test_num_cols):\n","  stdization_params = StandardScaler().fit(X_test[[X_test_cols[i]]])\n","  X_test[X_test_cols[i]] = stdization_params.transform(X_test[[X_test_cols[i]]])\n","\n","X_train_cols = X_train.columns\n","X_train_num_cols= X_train.shape[1]\n","for i in range(X_train_num_cols):\n","  stdization_params = StandardScaler().fit(X_train[[X_train_cols[i]]])\n","  X_train[X_train_cols[i]] = stdization_params.transform(X_train[[X_train_cols[i]]])"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9720,"status":"ok","timestamp":1716406865841,"user":{"displayName":"Soham Vaishnav","userId":"12778674038887209920"},"user_tz":-330},"id":"pbuRTqzH3GZE","outputId":"dc68bf35-4766-4ec9-f006-6e8bea1bbc96"},"outputs":[{"output_type":"stream","name":"stdout","text":["(56000, 2)\n","(14000, 784)\n"]}],"source":["#@title Performing PCA\n","\n","from sklearn.decomposition import PCA\n","import numpy as np\n","\n","#getting the cov matrix for train and test datasets\n","# X_train_cov = X_train.cov()\n","# X_test_cov = X_test.cov()\n","\n","#finding the eigen values and corresponding vectors in order to induce dimensional reduction\n","# eigenvalues_X_train, eigenvectors_X_train = np.linalg.eig(X_train_cov)\n","# eigenvalues_X_test, eigenvectors_X_test = np.linalg.eig(X_test_cov)\n","\n","#now using the sklearn library for PCA,\n","pca_params = PCA(n_components = 2).fit(X_train)\n","X_train_pca = pca_params.transform(X_train)\n","print(X_train_pca.shape)\n","X_test_pca = pca_params.transform(X_test)\n","print(X_test_pca.shape)\n","# print(pca_params.explained_variance_)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":577,"status":"ok","timestamp":1716380934099,"user":{"displayName":"Soham Vaishnav","userId":"12778674038887209920"},"user_tz":-330},"id":"cHbGQ5JI8WpP","outputId":"aa5153a2-d910-4751-b021-e4f626dfb151"},"outputs":[{"name":"stdout","output_type":"stream","text":["the score for number of neighbors =  3 is  100\n"]}],"source":["#@title Visualisation\n","\n","#un-comment the following code snippets to visualise the MNIST data after PCA\n","\n","# pca_params = PCA(n_components = 2).fit(X_train)\n","# X_train_pca = pca_params.transform(X_train)\n","# print(X_train_pca.shape)\n","# plt.scatter(X_train_pca[:,0], X_train_pca[:,1])\n","\n","# X_test_pca = pca_params.transform(X_test)\n","# print(X_test_pca.shape)\n","# plt.scatter(X_test_pca[:,0], X_test_pca[:,1])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50167,"status":"ok","timestamp":1716380075047,"user":{"displayName":"Soham Vaishnav","userId":"12778674038887209920"},"user_tz":-330},"id":"Dm1_P65E84iK","outputId":"9f644683-4263-4b59-fa71-c39ddbb46495"},"outputs":[{"name":"stdout","output_type":"stream","text":["(14000, 784)\n","0.9446428571428571\n"]}],"source":["#@title KNN Classification\n","\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","#performing KNN algo for n_neighbors = 3, 5, and 7\n","for k in range(3, 8, 2):\n","  print(X_test.shape)\n","  knn_model = KNeighborsClassifier(n_neighbors = k).fit(X_train_pca, y_train)\n","  score = knn_model.score(X_test_pca, y_test)\n","  print(\"the score for number of neighbors = \", k, \" is \", score*100)"]},{"cell_type":"markdown","metadata":{"id":"zUaBxIHPd5CC"},"source":["## Part 2: K-Means + SVM\n","\n","### Prelude\n","\n","K-Means is a popular clustering algorithm, and Support Vector Machines (SVMs) are a powerful classification method. In this part, you will use K-Means to extract features from the dataset and then use these features to train an SVM classifier.\n","\n","### Steps\n","\n","1. **K-Means Clustering:** Apply K-Means to find clusters in the dataset.\n","2. **Feature Extraction:** Use the distances from each point to the cluster centroids as features.\n","3. **SVM Classification:** Use the SVM classifier to classify the digits."]},{"cell_type":"markdown","metadata":{"id":"nsDJZFGDd5CC"},"source":["**Additional Tips for Students:**\n","- **Choosing the Number of Clusters (k) in K-Means:** Start with `k=10` since there are 10 digits (0-9). Experiment with different values to see if they improve the performance.\n","- **Selecting SVM Kernel:** Try different kernels like 'linear', 'poly', 'rbf', and 'sigmoid'. Observe how the choice of kernel affects accuracy.\n","- **Visualization:** Consider visualizing the centroids of the clusters. Each centroid is a point in the same space as the input data and can be viewed as an \"average\" digit if reshaped to 28x28 pixels.\n","- **Cross-Validation:** Use cross-validation to find the best parameters for both K-Means and SVM to further improve the model.\n","\n","---"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1363,"status":"ok","timestamp":1716406874777,"user":{"displayName":"Soham Vaishnav","userId":"12778674038887209920"},"user_tz":-330},"id":"REE7j4VcR31X","outputId":"50ea163d-362e-43b0-fc30-f603481b9924"},"outputs":[{"output_type":"stream","name":"stdout","text":["10\n","[[-4.60747694  2.89817624]\n"," [-6.35429956 -1.8407708 ]\n"," [17.09118319  3.87304771]\n"," [ 6.5324151   2.66554109]\n"," [ 0.27865082 -0.11941424]\n"," [14.60508153 -7.55014095]\n"," [ 5.56942685 12.2793274 ]\n"," [ 5.25491862 -6.43747573]\n"," [-0.90498198  7.46636009]\n"," [-1.68228201 -5.68414525]]\n","(56000, 10)\n"]}],"source":["#@title K-Means Clustering\n","\n","from sklearn.cluster import MiniBatchKMeans, KMeans\n","from sklearn.metrics import accuracy_score\n","import pandas as pd\n","\n","for n_clusters in range(10, 11):\n","  #setting the initial params\n","  n_init = 10\n","  algo_init = \"random\"\n","\n","  #K-means algo\n","  kmeans_params = KMeans(n_clusters = n_clusters, init = algo_init, n_init = n_init)\n","  kmeans_params.fit(X_train_pca)\n","\n","  #finding cluster centres and followed by SS wrt centroids for feature selection from K-Means\n","  cluster_centroids = kmeans_params.__dict__['cluster_centers_']\n","  features_selected_train = np.column_stack([np.sqrt(np.sum((X_train_pca-ci)**2, axis = 1)) for ci in cluster_centroids])\n","  features_selected_test = np.column_stack([np.sqrt(np.sum((X_test_pca-ci)**2, axis = 1)) for ci in cluster_centroids])"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"id":"pJ1CjlUy-1h0","executionInfo":{"status":"ok","timestamp":1716407183079,"user_tz":-330,"elapsed":303865,"user":{"displayName":"Soham Vaishnav","userId":"12778674038887209920"}},"outputId":"cbb6148b-9b5d-4385-c9cc-f58c2b38eda5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["SVC(C=1, kernel='poly')"],"text/html":["<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=1, kernel=&#x27;poly&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1, kernel=&#x27;poly&#x27;)</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":16}],"source":["#@title SVM Implementation\n","\n","from sklearn.svm import SVC\n","\n","svm_params = SVC(C = 1, kernel = 'poly', degree = 3)\n","svm_params.fit(features_selected_train, y_train)"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39007,"status":"ok","timestamp":1716407581188,"user":{"displayName":"Soham Vaishnav","userId":"12778674038887209920"},"user_tz":-330},"id":"A-IvFKvVPZ5X","outputId":"4adc8538-eed0-4f0a-bbe4-eb6899d9f84f"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.3712142857142857\n"]}],"source":["#@title Accuracy Score before Cross Validation\n","\n","from sklearn.metrics import accuracy_score\n","\n","y_pred = svm_params.predict(features_selected_test)\n","print(accuracy_score(y_test, y_pred))"]},{"cell_type":"code","source":["#@title Cross Validation\n","\n"],"metadata":{"id":"uR9q6F9O3YP_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3KKZF8dxd5CD"},"source":["## Part 3: SIFT + SVM\n","\n","### Prelude\n","\n","Scale-Invariant Feature Transform (SIFT) is an algorithm to detect and describe local features in images. After extracting these features, you will use an SVM classifier for the classification.\n","\n","### Steps\n","\n","1. **SIFT Feature Extraction:** Extract SIFT features from each image.\n","2. **Feature Description:** Use the features to describe the dataset.\n","3. **SVM Classification:** Use these descriptions to train and predict using SVM.\n","\n","### Starter Code\n","\n","#### SIFT Feature Extraction\n","\n","First, let's define a function to extract SIFT features from an image."]},{"cell_type":"markdown","metadata":{"id":"-zHKB848d5CD"},"source":["**Additional Tips for Students:**\n","- **SIFT Feature Size:** SIFT descriptors are 128-dimensional; ensure all feature vectors are the same length.\n","- **Choosing SVM Kernel:** Try 'linear', 'poly', 'rbf', and 'sigmoid' kernels to observe their effects.\n","- **Regularization Parameter (C):** Experiment with different values of \\(C\\); smaller values specify stronger regularization.\n","- **Handling Missing Descriptors:** In case no keypoints are found in an image, use a zero vector for that image’s descriptors.\n","\n","---"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}